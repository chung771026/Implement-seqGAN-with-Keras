{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Oct 12 12:04:31 2017\n",
    "\n",
    "@author: gama\n",
    "\"\"\"\n",
    "from  keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding,Masking\n",
    "from keras.layers import Input, Dense,Reshape,concatenate,Flatten,Activation,Permute,multiply\n",
    "from keras.layers import GRU,Conv1D,GlobalMaxPooling1D,TimeDistributed,RepeatVector,LSTM,MaxPooling1D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Lambda,Dropout\n",
    "from keras.utils import to_categorical,multi_gpu_model\n",
    "import gc\n",
    "import random\n",
    "import nltk\n",
    "import math\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import csv\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting tf_config\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=True))\n",
    "K.set_session(sess)\n",
    "# jieba.load_userdict('dict.txt.big.txt')\n",
    "# jieba.load_userdict('NameDict_Ch_v2.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "post=[]\n",
    "response=[]\n",
    "post_file=open('QA dataset/stc_weibo_train_post','r')\n",
    "for i in post_file.readlines():\n",
    "    post.append(i.split())\n",
    "\n",
    "post_file=open('QA dataset/stc_weibo_train_response','r')\n",
    "for i in post_file.readlines():\n",
    "    response.append(i.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4435959 4435959\n"
     ]
    }
   ],
   "source": [
    "print(len(post),len(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val\n",
    "token_stream = []\n",
    "que_pad=20\n",
    "ans_pad=10\n",
    "stop_count=0\n",
    "pair_train1=[]\n",
    "pair_train2=[]\n",
    "check_stop=[]\n",
    "count=0\n",
    "\n",
    "for key,i in enumerate(post):\n",
    "    if len(pair_train1)>=100000:\n",
    "        break\n",
    "    if len(i)>=ans_pad or len(response[key])>=ans_pad or len(i)<3 or len(response[key])<3:\n",
    "        continue\n",
    "    while len(i)<que_pad:\n",
    "        i.append('PAD')\n",
    "    while len(response[key])<ans_pad:\n",
    "        response[key].append('PAD')    \n",
    "    token_stream.extend(i)\n",
    "    pair_train1.append(i)    \n",
    "    token_stream.extend(response[key])\n",
    "    pair_train2.append(response[key])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_pairs 100000\n",
      "num_words:\n",
      "74020\n",
      "process_data\n"
     ]
    }
   ],
   "source": [
    "print('num_of_pairs',len(pair_train1))\n",
    "pair=len(pair_train1)\n",
    "#TOP=['PAD','EOS']             \n",
    "#TOP.extend(token_stream)\n",
    "words=list(set(token_stream))\n",
    "words.remove('PAD')\n",
    "#del token_stream\n",
    "\n",
    "word2idx={}\n",
    "word2idx['PAD']=0\n",
    "for i, word in enumerate(words):\n",
    "    word2idx[word]=i+1\n",
    "num_words = len(word2idx)\n",
    "print(\"num_words:\")\n",
    "print(num_words)\n",
    "                    \n",
    "print('process_data')\n",
    "\n",
    "predict_pair=ans_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pair_train1)):\n",
    "    for j in range(que_pad):\n",
    "        pair_train1[i][j]=word2idx[pair_train1[i][j]]\n",
    "\n",
    "for i in range(len(pair_train2)):\n",
    "    for j in range(ans_pad):\n",
    "        pair_train2[i][j]=word2idx[pair_train2[i][j]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[]\n",
    "train_y=[]\n",
    "pad_sequence=[word2idx['PAD']]*ans_pad\n",
    "for i in range(len(pair_train1)):\n",
    "    for j in range(ans_pad):\n",
    "        forward=pair_train1[i][:ans_pad]\n",
    "        backward=pad_sequence[j:ans_pad]\n",
    "        #print(backward)\n",
    "        train_x.append(forward+backward+pair_train2[i][:j]) \n",
    "        train_y.append([pair_train2[i][j]])\n",
    "\n",
    " \n",
    "train_x=np.array(train_x)\n",
    "train_y=np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 20)\n",
      "(1000000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "def get_model():\n",
    "    dim=256\n",
    "    inputs = Input(shape=(que_pad,))\n",
    "    g_emb=Embedding(num_words+1,dim,mask_zero=True, input_length=(que_pad))(inputs)\n",
    "    decoder = GRU(dim)(g_emb)\n",
    "    decoder = Dense(num_words,activation=\"softmax\")(decoder)\n",
    "    model = Model(inputs=inputs , outputs=decoder)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppx(y_true, y_pred):\n",
    "     loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "     perplexity = K.cast(K.pow(math.e, K.mean(loss, axis=-1)), K.floatx())\n",
    "     return perplexity\n",
    "   \n",
    "\n",
    "g_model=get_model()\n",
    "#sampling_model= multi_gpu_model(get_model(), gpus=2)\n",
    "g_model.compile(loss=ppx, optimizer='adam',metrics=['accuracy'])\n",
    "earlyStopping=EarlyStopping(monitor='loss', patience=2, verbose=2, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/49\n",
      "800000/800000 [==============================] - 144s 179us/step - loss: 1057.1888 - acc: 0.9978 - val_loss: 398.9773 - val_acc: 0.9991\n",
      "Epoch 2/49\n",
      "800000/800000 [==============================] - 141s 176us/step - loss: 303.1695 - acc: 0.5446 - val_loss: 311.7761 - val_acc: 0.3936\n",
      "Epoch 3/49\n",
      "800000/800000 [==============================] - 140s 175us/step - loss: 219.7435 - acc: 0.3908 - val_loss: 261.2840 - val_acc: 0.3614\n",
      "Epoch 4/49\n",
      "800000/800000 [==============================] - 140s 175us/step - loss: 163.2398 - acc: 0.3499 - val_loss: 268.2411 - val_acc: 0.2751\n",
      "Epoch 5/49\n",
      "800000/800000 [==============================] - 141s 176us/step - loss: 120.4709 - acc: 0.3249 - val_loss: 236.0833 - val_acc: 0.2838\n",
      "Epoch 6/49\n",
      "800000/800000 [==============================] - 140s 176us/step - loss: 84.8350 - acc: 0.3099 - val_loss: 237.2594 - val_acc: 0.2684\n",
      "Epoch 7/49\n",
      "800000/800000 [==============================] - 141s 176us/step - loss: 57.5407 - acc: 0.3004 - val_loss: 261.2726 - val_acc: 0.2500\n",
      "Epoch 8/49\n",
      "800000/800000 [==============================] - 141s 176us/step - loss: 39.2369 - acc: 0.2943 - val_loss: 273.9750 - val_acc: 0.2598\n",
      "Epoch 9/49\n",
      "800000/800000 [==============================] - 140s 175us/step - loss: 27.7128 - acc: 0.2901 - val_loss: 312.1299 - val_acc: 0.2382\n",
      "Epoch 10/49\n",
      "800000/800000 [==============================] - 140s 175us/step - loss: 20.7686 - acc: 0.2867 - val_loss: 352.9848 - val_acc: 0.2379\n",
      "Epoch 11/49\n",
      "800000/800000 [==============================] - 140s 176us/step - loss: 16.3218 - acc: 0.2843 - val_loss: 397.5277 - val_acc: 0.2261\n",
      "Epoch 12/49\n",
      "800000/800000 [==============================] - 141s 177us/step - loss: 13.2393 - acc: 0.2824 - val_loss: 439.9481 - val_acc: 0.2259\n",
      "Epoch 13/49\n",
      "800000/800000 [==============================] - 143s 179us/step - loss: 10.9876 - acc: 0.2806 - val_loss: 501.5279 - val_acc: 0.2189\n",
      "Epoch 14/49\n",
      "800000/800000 [==============================] - 140s 175us/step - loss: 9.2822 - acc: 0.2793 - val_loss: 532.2769 - val_acc: 0.2217\n",
      "Epoch 15/49\n",
      "799744/800000 [============================>.] - ETA: 0s - loss: 7.9793 - acc: 0.2782"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-7c81c5ad835c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g_model.fit(train_x,train_y, epochs=49, batch_size=512,validation_split=0.2,verbose=1,callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model=g_model #prepare assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "学校发的蚊帐终于派上用场了PADPADPADPADPADPADPADPADPADPADPADPAD我们高中的时候就用上了PADPADPAD\n",
      "RAP_model\n",
      "我也有一个，有木有。\n",
      "question\n",
      "分享图片老板，再来一桶PADPADPADPADPADPADPADPADPADPADPADPADPAD哈哈哈。蒙到无语了。喜悦啊。PAD\n",
      "RAP_model\n",
      "一个字——不要这么搞。PADPAD\n",
      "question\n",
      "不要相信谣言，因为真相更惊人。PADPADPADPADPADPADPADPADPADPADPAD谣言止于智者，智者却要止于真相PAD\n",
      "RAP_model\n",
      "精辟，精辟，方舟子。PADPADPADPAD\n",
      "question\n",
      "他妈的，这战术，坑爹啊。PADPADPADPADPADPADPADPADPADPADPAD唯一需要用到的技术就是淡定……PAD\n",
      "RAP_model\n",
      "这是什么意思的名字PADPADPADPAD\n",
      "question\n",
      "【娱乐一下】男生的价格计算方法PADPADPADPADPADPADPADPADPADPADPAD原来只是正常而已？！哈哈哈。PADPAD\n",
      "RAP_model\n",
      "关键是男性错误主义价值。PADPADPAD\n"
     ]
    }
   ],
   "source": [
    "def output_sequence(pair_train1,pair_train2,num,g_model):\n",
    "    word2=[]\n",
    "    test=[pair_train1[num]]\n",
    "    #print(test)\n",
    "    test=np.array(test)\n",
    "    index=g_model.predict(test)\n",
    "    index=np.argmax(index[0],axis=-1)      \n",
    "    word2.append(index)\n",
    "    for i in range(ans_pad-1):\n",
    "        test=np.delete(test,ans_pad,1)\n",
    "        test=np.concatenate([test,[[index]]],axis=1)\n",
    "        index=g_model.predict(test)\n",
    "        index=np.argmax(index[0],axis=-1)\n",
    "        word2.append(index)\n",
    "        if str(index) == str(word2idx['EOS']):\n",
    "              break\n",
    "    que=[]\n",
    "    sample=[]\n",
    "    test=[pair_train1[num]+pair_train2[num]]\n",
    "    for g in test[0]:\n",
    "          for value, age in word2idx.items():\n",
    "                if age == g:\n",
    "                \tque.append(value)\n",
    "    for g in word2:\n",
    "          for value, age in word2idx.items():\n",
    "                if age == g:\n",
    "                \tsample.append(value)\n",
    "    print('question')\n",
    "    print(''.join(que))\n",
    "    print('RAP_model') \n",
    "    print(''.join(sample))\n",
    "    que=que[0:20]+['   ans:   ']+que[ans_pad:]      \n",
    "    return  ''.join(que),''.join(sample)\n",
    "update_g=len(pair_train1)\n",
    "for i in range(5):\n",
    "    output_sequence(pair_train1,pair_train2,random.randint(0,random.randint(0,pair-1)),g_model)\n",
    "\n",
    "old_result=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=128\n",
    "d_input1=Input(shape=(que_pad,))\n",
    "d_input2=Input(shape=(1,))\n",
    "#,mask_zero=True\n",
    "con=concatenate([d_input1,d_input2],axis=1)\n",
    "d_emb=Embedding(num_words+1,dim, input_length=(que_pad+1))(con)\n",
    "sent_representation=LSTM(dim)(d_emb)\n",
    "#sent_representation=GlobalMaxPooling1D()(cnn)\n",
    "probabilities = Dense(2, activation='softmax')(sent_representation)\n",
    "discriminator = Model([d_input1,d_input2] , probabilities)\n",
    "discriminator.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_index(fake):\n",
    "    fake_idx=[]\n",
    "    for i in fake:\n",
    "    \tw=np.argmax(i)\n",
    "    \tfake_idx.append([w])\n",
    "    fake_idx=np.array(fake_idx)\n",
    "    return fake_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_reward(train_x,train_y,count,candidate):\n",
    "    rollout_sample=[]\n",
    "    new_input=np.array(train_x)\n",
    "    #print(new_input.shape)\n",
    "    new_x=[]\n",
    "    new_y=[]\n",
    "    new_input_x=[]\n",
    "    new_input_y=[]\n",
    "    reward=[]\n",
    "    \n",
    "    for i in range(count):\n",
    "        fake=g_model.predict(new_input)\n",
    "        old_input=new_input\n",
    "        new_input=[]\n",
    "        for k in range(len(fake)):\n",
    "            #print(k)\n",
    "            index=np.argsort(fake[k])\n",
    "            index=index[::-1]#排序由機率大到小\n",
    "            for key,j in enumerate(index[0:candidate]):\n",
    "                if i==0:\n",
    "                    new_input_x.append(train_x[0])\n",
    "                    new_input_y.append([j])\n",
    "                new_x.append(new_input)\n",
    "                new_y.append([j])\n",
    "                con=np.delete(old_input[k],ans_pad,0)\n",
    "                con=np.concatenate([con,[j]],axis=0)\n",
    "                new_input.append(con)\n",
    "                \n",
    "        new_input=np.array(new_input)\n",
    "        \n",
    "    new_y=np.array(new_y)\n",
    "    #print(new_input,new_y) \n",
    "    k=len(new_input)//candidate\n",
    "    for i in range(candidate):\n",
    "        vector=discriminator.predict([new_input[k*i:k*(i+1)],\n",
    "                                  new_y[k*i:k*(i+1)]])[:,0]\n",
    "        reward.append(np.mean(vector))\n",
    "#     reward.append(discriminator.predict([train_x,train_y])[0][0])\n",
    "    #print(reward)\n",
    "    reward=reward-np.mean(reward)\n",
    "#     new_input_x.append(train_x[0])\n",
    "#     new_input_y.append(train_y[0])\n",
    "    return new_input_x,new_input_y,reward.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(g_model,train_x,train_y):\n",
    "    new_trainX=[]\n",
    "    new_trainY=[]\n",
    "    for key,i in enumerate(tqdm_notebook(train_x)):\n",
    "        if key%ans_pad==0:\n",
    "            train_w=i\n",
    "        else:\n",
    "            #train_w[ans_pad+key%ans_pad-1]=index\n",
    "            train_w=np.delete(train_w,ans_pad,0)   \n",
    "            train_w=np.concatenate([train_w,[index]],axis=0)    \n",
    "        index=g_model.predict(np.array([train_w]))\n",
    "        index=np.argmax(index[0],axis=-1)\n",
    "        new_trainX.append(train_w)    \n",
    "        new_trainY.append([index])\n",
    "    new_trainX=np.array(new_trainX)\n",
    "    new_trainY=np.array(new_trainY)\n",
    "    return new_trainX,new_trainY\n",
    "def train_d(discriminator,train_x,train_y,X,Y):\n",
    "    earlyStopping=EarlyStopping(monitor='loss', patience=1, verbose=2, mode='auto')\n",
    "    n = len(train_x)\n",
    "    YT = np.zeros([n*2,2])\n",
    "    YT[0:n,1] = 1\n",
    "    YT[n:,0] = 1\n",
    "    #fake=distribution_index(fake)\n",
    "    XT=np.concatenate([X,train_x])\n",
    "    XT2=np.concatenate([Y,train_y])\n",
    "    result=discriminator.fit([XT,XT2],YT, epochs=1,shuffle=True, batch_size=32, verbose=0,callbacks=[earlyStopping])\n",
    "    return result.history['acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_all_mean(train_x,train_y):\n",
    "    fake=g_model.predict(train_x)\n",
    "    #per=s_model.evaluate(train_x,train_y,batch_size=512)\n",
    "\n",
    "    #per=1/per[0]\n",
    "    mean=discriminator.predict([train_x,distribution_index(fake)])\n",
    "    return np.mean(mean[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa57ebc4d5e43668ea248da90532d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-fd666e37035e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mans_pad\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtrain_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_w\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnew_trainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch=0\n",
    "#for s in range(120):\n",
    "for x in range(10):\n",
    "#     print(\"greedy-search\")\n",
    "#     X,Y=greedy_search(g_model,train_x[::ans_pad],train_y[::ans_pad])\n",
    "#     print('d-step')\n",
    "#     result=train_d(discriminator,train_x[::ans_pad],train_y[::ans_pad],X,Y)\n",
    "#     del X,Y\n",
    "#     print(result)\n",
    "\n",
    "#     print('finishsearch')\n",
    "    count=0   \n",
    "    dis_x=[]\n",
    "    dis_y=[]\n",
    "    reward=[]\n",
    "    for g in tqdm_notebook(range(int(len(train_x)))):\n",
    "        if g%ans_pad==0:\n",
    "             code=0\n",
    "        new_trainX=[]\n",
    "        new_trainY=[]\n",
    "        if g%ans_pad==0:\n",
    "            train_w=train_x[g]    \n",
    "        index=g_model.predict(np.array([train_w]))\n",
    "        index=np.argmax(index[0],axis=-1)\n",
    "        new_trainX.append(train_w)    \n",
    "        new_trainY.append([index])\n",
    "        if g%ans_pad!=0:\n",
    "            train_w=np.delete(train_w,ans_pad,0)   \n",
    "            train_w=np.concatenate([train_w,[index]],axis=0)\n",
    "        s=rollout_reward(new_trainX,new_trainY,(ans_pad-g%ans_pad),2)    \n",
    "        dis_x.extend(s[0])\n",
    "        dis_y.extend(s[1]) \n",
    "        reward.extend(s[2])\n",
    "        code+=1 \n",
    "        if g%10000==0 and g!=0:\n",
    "            print('batch',g)\n",
    "            g_model.fit(np.array(dis_x),np.array(dis_y),epochs=1,batch_size=256,sample_weight=np.array(reward),verbose=0)\n",
    "            dis_x=[]\n",
    "            dis_y=[]\n",
    "            reward=[]\n",
    "            for i in range(3):\n",
    "                output_sequence(pair_train1,pair_train2,random.randint(0,random.randint(0,pair-1)),g_model)\n",
    "    epoch=epoch+1\n",
    "    print('epoch',epoch) \n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "有移动互联网记者和新媒体编辑跳槽么PADPADPADPADPADPADPADPADPADPADPAD可考虑啊在上海吗待遇怎样PADPAD\n",
      "RAP_model\n",
      "的感觉好有爱啊，好有趣PAD\n",
      "question\n",
      "40年前的数学课本，很给力！PADPADPADPADPADPADPADPADPADPADPAD仔细一看，确实给力PADPADPADPAD\n",
      "RAP_model\n",
      "地主阶级是巧合的！PADPADPADPAD\n",
      "question\n",
      "据说Linsanity有官方翻译了：林彪！PADPADPADPADPADPADPADPADPADPADPAD那他未来儿子起名…Linfruit：林立果PAD\n",
      "RAP_model\n",
      "东北人在中国运动员不要说话PADPADPADPAD\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    output_sequence(pair_train1,pair_train2,random.randint(0,random.randint(0,pair-1)),g_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
